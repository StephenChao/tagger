{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 18:01:11.921195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-27 18:01:11.921239: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/pku/zhaoyz/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import boost_histogram as bh\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from scipy import interpolate\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from cycler import cycler\n",
    "import awkward as ak\n",
    "\n",
    "import uproot ## means uproot4\n",
    "import sklearn.metrics as m\n",
    "# from keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_helvet = False ## true: use helvetica for plots, make sure the system have the font installed\n",
    "if use_helvet:\n",
    "    CMShelvet = hep.style.CMS\n",
    "    CMShelvet['font.sans-serif'] = ['Helvetica', 'Arial']\n",
    "    plt.style.use(CMShelvet)\n",
    "else:\n",
    "    plt.style.use(hep.style.CMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CustNanoData = {\n",
    "    'Data' : \"/data/pubfs/zhaoyz/Tree/V5/Merged/2018/Data/Tree_Data.root\",            \n",
    "    'QCD' : \"/data/pubfs/zhaoyz/Tree/V5/Merged/2018/MC/Tree_QCD.root\",   \n",
    "    'BKG' : \"/data/pubfs/zhaoyz/Tree/V5/Merged/2018/MC/Tree_BKG.root\",           \n",
    "    'Signal' : \"/data/pubfs/zhaoyz/Tree/V5/Merged/2018/Signal_Nomatching/Tree_Total.root\",\n",
    "\n",
    "    'GluGlu' : \"/data/pubfs/zhaoyz/Tree/V5/Merged/2018/Signal_Nomatching/Tree_GluGlu.root\",            \n",
    "    'VH' : \"/data/pubfs/zhaoyz/Tree/V5/Merged/2018/Signal_Nomatching/Tree_VH.root\",            \n",
    "    'VBF' : \"/data/pubfs/zhaoyz/Tree/V5/Merged/2018/Signal_Nomatching/Tree_VBF.root\",            \n",
    "    'ttH' : \"/data/pubfs/zhaoyz/Tree/V5/Merged/2018/Signal_Nomatching/Tree_ttH.root\",            \n",
    "            \n",
    "}\n",
    "events = {typefile : {} for typefile in CustNanoData}\n",
    "for typefile in CustNanoData:\n",
    "    events[typefile] = uproot.lazy({CustNanoData[typefile]: \"PKUTree\" }) ## lazy means lazy computation style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to combine the \"signal\" and \"BKG\" together.\n",
    "def combine_events(eventsSignal, eventsBKG, split_step_signal = 1, split_step_bkg = 1):\n",
    "    eventsSignal[\"isSignal\"] = 1\n",
    "    eventsBKG[\"isSignal\"] = 0\n",
    "    events_combine = {branch : ak.highlevel.Array for branch in eventsSignal.fields if (\"hid\" in branch or \"isSignal\" in branch)}\n",
    "    #only the branch with \"hid\" and \"isSignal\" is needed.\n",
    "    for branch in events_combine.keys():\n",
    "        events_combine[branch] =  ak.concatenate([eventsSignal[branch][::split_step_signal],  eventsBKG[branch][::split_step_bkg]], highlevel=True)\n",
    "        #Select the signal events and bkg events every split_step_signal'th events, and then connect the signal and background together.\n",
    "    return events_combine\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_combine = combine_events(events[\"Signal\"],events[\"BKG\"], split_step_signal = 10, split_step_bkg = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(events_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "awkward.highlevel.Array"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(events_combine[\"isSignal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "awkward.highlevel.Array"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(events_combine[\"a_hidNeuron000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['events_combine[\"a_hidNeuron000\"]', 'events_combine[\"a_hidNeuron001\"]', 'events_combine[\"a_hidNeuron002\"]', 'events_combine[\"a_hidNeuron003\"]', 'events_combine[\"a_hidNeuron004\"]', 'events_combine[\"a_hidNeuron005\"]', 'events_combine[\"a_hidNeuron006\"]', 'events_combine[\"a_hidNeuron007\"]', 'events_combine[\"a_hidNeuron008\"]', 'events_combine[\"a_hidNeuron009\"]', 'events_combine[\"a_hidNeuron010\"]', 'events_combine[\"a_hidNeuron011\"]', 'events_combine[\"a_hidNeuron012\"]', 'events_combine[\"a_hidNeuron013\"]', 'events_combine[\"a_hidNeuron014\"]', 'events_combine[\"a_hidNeuron015\"]', 'events_combine[\"a_hidNeuron016\"]', 'events_combine[\"a_hidNeuron017\"]', 'events_combine[\"a_hidNeuron018\"]', 'events_combine[\"a_hidNeuron019\"]', 'events_combine[\"a_hidNeuron020\"]', 'events_combine[\"a_hidNeuron021\"]', 'events_combine[\"a_hidNeuron022\"]', 'events_combine[\"a_hidNeuron023\"]', 'events_combine[\"a_hidNeuron024\"]', 'events_combine[\"a_hidNeuron025\"]', 'events_combine[\"a_hidNeuron026\"]', 'events_combine[\"a_hidNeuron027\"]', 'events_combine[\"a_hidNeuron028\"]', 'events_combine[\"a_hidNeuron029\"]', 'events_combine[\"a_hidNeuron030\"]', 'events_combine[\"a_hidNeuron031\"]', 'events_combine[\"a_hidNeuron032\"]', 'events_combine[\"a_hidNeuron033\"]', 'events_combine[\"a_hidNeuron034\"]', 'events_combine[\"a_hidNeuron035\"]', 'events_combine[\"a_hidNeuron036\"]', 'events_combine[\"a_hidNeuron037\"]', 'events_combine[\"a_hidNeuron038\"]', 'events_combine[\"a_hidNeuron039\"]', 'events_combine[\"a_hidNeuron040\"]', 'events_combine[\"a_hidNeuron041\"]', 'events_combine[\"a_hidNeuron042\"]', 'events_combine[\"a_hidNeuron043\"]', 'events_combine[\"a_hidNeuron044\"]', 'events_combine[\"a_hidNeuron045\"]', 'events_combine[\"a_hidNeuron046\"]', 'events_combine[\"a_hidNeuron047\"]', 'events_combine[\"a_hidNeuron048\"]', 'events_combine[\"a_hidNeuron049\"]', 'events_combine[\"a_hidNeuron050\"]', 'events_combine[\"a_hidNeuron051\"]', 'events_combine[\"a_hidNeuron052\"]', 'events_combine[\"a_hidNeuron053\"]', 'events_combine[\"a_hidNeuron054\"]', 'events_combine[\"a_hidNeuron055\"]', 'events_combine[\"a_hidNeuron056\"]', 'events_combine[\"a_hidNeuron057\"]', 'events_combine[\"a_hidNeuron058\"]', 'events_combine[\"a_hidNeuron059\"]', 'events_combine[\"a_hidNeuron060\"]', 'events_combine[\"a_hidNeuron061\"]', 'events_combine[\"a_hidNeuron062\"]', 'events_combine[\"a_hidNeuron063\"]', 'events_combine[\"a_hidNeuron064\"]', 'events_combine[\"a_hidNeuron065\"]', 'events_combine[\"a_hidNeuron066\"]', 'events_combine[\"a_hidNeuron067\"]', 'events_combine[\"a_hidNeuron068\"]', 'events_combine[\"a_hidNeuron069\"]', 'events_combine[\"a_hidNeuron070\"]', 'events_combine[\"a_hidNeuron071\"]', 'events_combine[\"a_hidNeuron072\"]', 'events_combine[\"a_hidNeuron073\"]', 'events_combine[\"a_hidNeuron074\"]', 'events_combine[\"a_hidNeuron075\"]', 'events_combine[\"a_hidNeuron076\"]', 'events_combine[\"a_hidNeuron077\"]', 'events_combine[\"a_hidNeuron078\"]', 'events_combine[\"a_hidNeuron079\"]', 'events_combine[\"a_hidNeuron080\"]', 'events_combine[\"a_hidNeuron081\"]', 'events_combine[\"a_hidNeuron082\"]', 'events_combine[\"a_hidNeuron083\"]', 'events_combine[\"a_hidNeuron084\"]', 'events_combine[\"a_hidNeuron085\"]', 'events_combine[\"a_hidNeuron086\"]', 'events_combine[\"a_hidNeuron087\"]', 'events_combine[\"a_hidNeuron088\"]', 'events_combine[\"a_hidNeuron089\"]', 'events_combine[\"a_hidNeuron090\"]', 'events_combine[\"a_hidNeuron091\"]', 'events_combine[\"a_hidNeuron092\"]', 'events_combine[\"a_hidNeuron093\"]', 'events_combine[\"a_hidNeuron094\"]', 'events_combine[\"a_hidNeuron095\"]', 'events_combine[\"a_hidNeuron096\"]', 'events_combine[\"a_hidNeuron097\"]', 'events_combine[\"a_hidNeuron098\"]', 'events_combine[\"a_hidNeuron099\"]', 'events_combine[\"a_hidNeuron100\"]', 'events_combine[\"a_hidNeuron101\"]', 'events_combine[\"a_hidNeuron102\"]', 'events_combine[\"a_hidNeuron103\"]', 'events_combine[\"a_hidNeuron104\"]', 'events_combine[\"a_hidNeuron105\"]', 'events_combine[\"a_hidNeuron106\"]', 'events_combine[\"a_hidNeuron107\"]', 'events_combine[\"a_hidNeuron108\"]', 'events_combine[\"a_hidNeuron109\"]', 'events_combine[\"a_hidNeuron110\"]', 'events_combine[\"a_hidNeuron111\"]', 'events_combine[\"a_hidNeuron112\"]', 'events_combine[\"a_hidNeuron113\"]', 'events_combine[\"a_hidNeuron114\"]', 'events_combine[\"a_hidNeuron115\"]', 'events_combine[\"a_hidNeuron116\"]', 'events_combine[\"a_hidNeuron117\"]', 'events_combine[\"a_hidNeuron118\"]', 'events_combine[\"a_hidNeuron119\"]', 'events_combine[\"a_hidNeuron120\"]', 'events_combine[\"a_hidNeuron121\"]', 'events_combine[\"a_hidNeuron122\"]', 'events_combine[\"a_hidNeuron123\"]', 'events_combine[\"a_hidNeuron124\"]', 'events_combine[\"a_hidNeuron125\"]', 'events_combine[\"a_hidNeuron126\"]', 'events_combine[\"a_hidNeuron127\"]']\n"
     ]
    }
   ],
   "source": [
    "# We want to creat an array like [events[\"hid000\"],... , events[\"hid127\"]], so we can use it as the input of neural network.\n",
    "DefineBranch = \"events_combine[\\\"a_hidNeuron000\\\"]\"\n",
    "X_list = []\n",
    "for i in range(128):\n",
    "    if i < 10:\n",
    "        vari = (DefineBranch.replace(\"000\",\"00\" + str(i)))\n",
    "    elif i < 100:\n",
    "        vari = (DefineBranch.replace(\"000\",\"0\" + str(i)))\n",
    "    else:\n",
    "        vari = (DefineBranch.replace(\"000\",str(i)))\n",
    "    X_list.append(vari)\n",
    "print(X_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [eval(i) for i in X_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = events_combine[\"isSignal\"]\n",
    "#We want to use y as the training output label, 1 = signal, 0 = bkg."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to train a multi input NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 18:09:53.464183: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-27 18:09:53.464213: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-27 18:09:53.464241: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (farm.phy.pku.edu.cn): /proc/driver/nvidia/version does not exist\n",
      "2022-12-27 18:09:53.464490: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'awkward.highlevel.Array'>\"}), <class 'awkward.highlevel.Array'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39min_list, outputs\u001b[39m=\u001b[39mout)\n\u001b[1;32m     14\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m model\u001b[39m.\u001b[39;49mfit(X, y, epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m _, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X, y)\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (accuracy\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py:984\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    981\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m    982\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m    983\u001b[0m   \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m--> 984\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    987\u001b[0m           _type_name(x), _type_name(y)))\n\u001b[1;32m    988\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    993\u001b[0m           adapter_cls, _type_name(x), _type_name(y)))\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'awkward.highlevel.Array'>\"}), <class 'awkward.highlevel.Array'>"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "a = [\"input%s = keras.layers.Input(shape=(1,))\"%(i) for i in range(128)]\n",
    "for i in a:\n",
    "    exec(i)\n",
    "# define the input000-input127 one by one.\n",
    "\n",
    "in_list = [eval(\"input%s\"%(i)) for i in range(128)]\n",
    "middle_concatenation = keras.layers.concatenate(in_list, axis=1)\n",
    "# Connect input000-input127 together to form a list as input layer.\n",
    "\n",
    "added1 = keras.layers.Dense(8, activation='relu')(middle_concatenation)\n",
    "out = keras.layers.Dense(1, activation='sigmoid')(added1)\n",
    "\n",
    "model = keras.models.Model(inputs=in_list, outputs=out)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=30, batch_size=1000) #start to train.\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "model.save(\"HWWmodel_Method2_MultiInput_4.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# load model\n",
    "model_loaded = load_model('HWWmodel.h5')\n",
    "# summarize model.\n",
    "model_loaded.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42b66ccadc3dcb2a548c5829117ed01e6a6676879497c63ab523657f90856c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
